---
title: "Text analysis with quanteda - Presentation"
author: "Federico Mammana & Kathryn Malchow"
date: "24/10/2021"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## What is quanteda?
quanteda is an R package to perform a variety of natural language processing tasks: corpus management, tokenization, analysis, visualization.
→ quanteda is to text analysis what dplyr and tidyr are to data wrangling

In quanteda, text is processed as:
corpus → after having converted text data in corpus format (through the corpus function) we can work on it with quanteda; a corpus holds documents separately from each other.
document-feature matrix (“dfm”) → the analytical unit in which perform analysis; text documents are organized in matrices, with original texts as rows and features as columns. “Features” are more generally defined than “terms”, as they may be raw terms, stemmed terms, terms without stopwords, etc.
tokens → usually each word in a text, but also single characters or sentences if we want.
Insert picture of DFM here?
## Why use quanteda?
quanteda is built to be faster and more efficient than any other R or Python package for processing large textual data. Infrastructure on three main pillars:
stringi package for text processing,
Matrix package for sparse matrix objects,
computationally intensive processing (e.g. for tokens) handled in parallelized C++

Intuitive, powerful, and flexible


## 

Now lets see text preprocessing workflow and some functions!

## Loading Text Data into R

library(readtext) - companion package to Quanteda to read text (.txt) files or comma-separated-value (.csv) files
library(quanteda)

Read in text:

speech_text = readtext(".....txt"),
  docvarsfrom = "filenames",
  docvarnames = c("country", "session", "year"),
  dvsep = "_")


Create a corpus:

speech_corpus = corpus(speech_text)


## Preprocessing - Tokenize the text

```{r, echo = TRUE}
text <- c("I <3 little pumpkins! OMG they're so cute.")


tokens(text, what = "sentence")
tokens(text, what = "character")
text_token = tokens(text, what = "word") #default
```
## Tokenize the text continued

```{r}
tokens(text,
remove_punct = TRUE,
remove_numbers = FALSE,
remove_symbols = TRUE)
```


##  Make lower case

```{r pressure}
# char_tolower(text) before tokenizing

tokens_tolower(text_token) #after tokenizing
```
## You can also keep acronyms by adding: keep_acronyms = TRUE

```{r}
#char_tolower(text, keep_acronyms = TRUE)

tokens_tolower(text_token, keep_acronyms = TRUE)
```
## Removing Stop Words

```{r}
tokens_remove(text_token, stopwords("en"))

```
## Removing other unwanted words

```{r}
tokens_remove(text_token, c("pumpkins", "cute"))
```
## Word Stemming

```{r}

dip_text = c("When I dip, he dips, we are dipping.")

dip_token = tokens(rand_text)

tokens_wordstem(dip_token)
```
## KeyWords in Context, a cool function

```{r}
fc_text = c("“You are not your job, you're not how much money you have in the bank. You are not the car you drive. You're not the contents of your wallet. You are not your fucking khakis. You are all singing, all dancing crap of the world.")

fc_token = tokens(fc_text)

kwic(fc_token, "you", window=2)

```
## Document Feature Matrix

```{r}
dfm()
```

# DFM Subset

```{r}
test
```

## DFM Trim
```{r}

```
